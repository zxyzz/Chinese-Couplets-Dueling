{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#https://github.com/mozillazg/python-pinyin\n",
    "from pypinyin import pinyin, lazy_pinyin, Style\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from nltk.translate import bleu\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(file_path):\n",
    "    # load vocab.txt with \\n at each line\n",
    "    f = open(file_path,\"r\")\n",
    "    lines = f.readlines()\n",
    "    \n",
    "    vocab = data_clean(lines)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def load_data(in_path, out_path, nb_couplets,with_all_couplets):\n",
    "    couplet_in = open(in_path, \"r\")\n",
    "    couplet_out = open(out_path, \"r\")\n",
    "\n",
    "    # load original couplets\n",
    "    lines_in = couplet_in.readlines()  # len = 770491\n",
    "    lines_out = couplet_out.readlines()\n",
    "\n",
    "    # clean samples\n",
    "    samples_in = data_clean(lines_in)\n",
    "    samples_out = data_clean(lines_out)\n",
    "\n",
    "    # determine min and max length\n",
    "    min_length = len(min(samples_in, key=len))\n",
    "    max_length = len(max(samples_out, key=len))\n",
    "    max_length +=  1\n",
    "    print(\"min line length is \", min_length)\n",
    "    print(\"max line length is \", max_length)\n",
    "    # define data_in and data_out   => X and Ŷ  \n",
    "    if (with_all_couplets == False):\n",
    "        samples_in = samples_in[:nb_couplets]  # take some samples\n",
    "        samples_out = samples_out[:nb_couplets]\n",
    "\n",
    "    print(\"Loaded input couplets length is \",len(samples_in))\n",
    "    print(\"Loaded output couplets length is \",len(samples_out))\n",
    "\n",
    "    return samples_in, samples_out, max_length\n",
    "\n",
    "\n",
    "# clean data\n",
    "def data_clean(data):\n",
    "    s=[]\n",
    "    for i in range(0, len(data)):\n",
    "        s.append(str(data[i]).replace('\\n','').replace(' ','')) \n",
    "    return s\n",
    "\n",
    "\n",
    "def data_clean_II(data):\n",
    "    s=[]\n",
    "    for i in range(0, len(data)):\n",
    "        #l= str(data[i]).replace('\\n','').strip('')\n",
    "        l= str(data[i]).strip('\\n').strip('？').strip('：').replace(' ','')\n",
    "        if (l.find('，') == -1):\n",
    "            s.append(l)\n",
    "        else:\n",
    "            l = l.split('，')\n",
    "            for j in range(0, len(l)):  # seperate couplets ....................how to consider , ? !   :tjs there\n",
    "                s.append(l[j])\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########\n",
    "\n",
    "# create char2int dictionary\n",
    "def create_char2int_and_int2char_dict(vocab):\n",
    "    rang = np.arange(len(vocab))\n",
    "    char2int_dict = dict(zip(vocab,rang))\n",
    "    int2char_dict = dict(zip(rang,vocab))\n",
    "    int2char_dict[0] = ''  # '' for pad\n",
    "    return char2int_dict,int2char_dict\n",
    "\n",
    "\n",
    "# create char2tone dictionary\n",
    "def create_char2tone_dict(vocab):\n",
    "    tone_dict = dict()\n",
    "    tone_dict[vocab[0]]= ' '\n",
    "    tone_dict[vocab[1]]= ' '\n",
    "    tone_dict[vocab[2]]= ' '\n",
    "    tone_dict[vocab[3]]= ' '\n",
    "    for i in range(4,len(vocab)): # modify 3------------------------\n",
    "        key = str(vocab[i])\n",
    "        #print(\"key is\", key)\n",
    "        t = pinyin(key, style=Style.TONE3, heteronym=True)[0] # tones for the character\n",
    "        #print(\"t is \", t)\n",
    "        for j in range(len(t)):\n",
    "            \n",
    "            pz = str(t[j][-1])  # tone number\n",
    "            #print(\"j\",j)\n",
    "            #print(\"pz \",pz)\n",
    "            if ( pz!= (key) and pz.isdigit() ):  \n",
    "                #print(\"-\")\n",
    "                if( int(pz) <= 2):\n",
    "                    #print(\"--\")\n",
    "                    if (j==0):\n",
    "                        #print(\"---\")\n",
    "                        tone_dict[key]=['P']\n",
    "                        #print(tone_dict[key])\n",
    "                    else:\n",
    "                        tone_dict[key].append('P')  \n",
    "                else:\n",
    "                    if(j==0):\n",
    "                        tone_dict[key]=['Z']\n",
    "                    else:\n",
    "                        tone_dict[key].append('Z')\n",
    "            else: \n",
    "                if (tone_dict.get(key) == None):\n",
    "                    tone_dict[key]=[' ']  # when there is no tone     # ， ？ ： 、 ； ！ - 屷 … 俬 〇 牗 -----\n",
    "                \n",
    "        \n",
    "        tone_dict[key] = set(tone_dict[key])\n",
    "        #print(\"tone_dict[key] set\",tone_dict[key])\n",
    "        tone_dict[key] = list(tone_dict[key])\n",
    "        #print(\"tone_dict[key] list\",tone_dict[key])\n",
    "        res =''\n",
    "        for k in range(len(tone_dict[key])):\n",
    "            res += tone_dict[key][k]\n",
    "        tone_dict[key] = res\n",
    "    return tone_dict\n",
    "\n",
    "\n",
    "    \n",
    "# tokenize : couplet to int vectors\n",
    "def tokenize(data, char2int_dict, max_length):  # data is sets of couplets\n",
    "    v=[]\n",
    "    for i in range(0, len(data)):\n",
    "        l = []\n",
    "        for j in range (0, len(data[i])):\n",
    "            if ( char2int_dict.get(data[i][j])  is None ):\n",
    "                l.append(   char2int_dict.get('UNK')  )\n",
    "            else:\n",
    "                l.append(   char2int_dict.get(data[i][j])  )\n",
    "        if (len(data[i]) < max_length):   # padding 0(pad) at the end\n",
    "            l = (l + [0] * max_length)[:max_length]\n",
    "        #v.append(torch.tensor(l, dtype=torch.long, device=device).view(-1, 1))\n",
    "        v.append(l)\n",
    "    return v\n",
    "\n",
    "\n",
    "def tokenize_dec_in(data, char2int_dict, max_length):  # data is sets of couplets\n",
    "    v=[]\n",
    "    for i in range(0, len(data)):\n",
    "        l = [char2int_dict.get('sos')]\n",
    "\n",
    "        for j in range (0, len(data[i])):\n",
    "            if ( char2int_dict.get(data[i][j])  is None ):\n",
    "                l.append(   char2int_dict.get('UNK')  )\n",
    "            else:\n",
    "                l.append(   char2int_dict.get(data[i][j])  )\n",
    "        if (len(data[i]) < max_length):   # padding 0(pad) at the end\n",
    "            l = (l + [0] * max_length)[:max_length]\n",
    "        v.append(l)\n",
    "        #v.append(torch.tensor(l, dtype=torch.long, device=device).view(-1, 1))\n",
    "    return v\n",
    "\n",
    "\n",
    "def tokenize_dec_out(data, char2int_dict, max_length):  # data is sets of couplets\n",
    "    v=[]\n",
    "    for i in range(0, len(data)):\n",
    "        l = []\n",
    "        for j in range (0, len(data[i])):\n",
    "            if ( char2int_dict.get(data[i][j])  is None ):\n",
    "                l.append(   char2int_dict.get('UNK')  )\n",
    "            else:\n",
    "                l.append(   char2int_dict.get(data[i][j])  )\n",
    "\n",
    "        l.append(char2int_dict.get('eos'))\n",
    "        if (len(data[i]) < max_length):   # padding 0(pad) at the end\n",
    "            l = (l + [0] * max_length)[:max_length]\n",
    "        #v.append(torch.tensor(l, dtype=torch.long, device=device).view(-1, 1))\n",
    "        v.append(l)\n",
    "    return v\n",
    "\n",
    "\n",
    "##########\n",
    "\n",
    "def detokenize(one_couplet, int2char_dict):\n",
    "    o = ''\n",
    "    for i in range(0, len(one_couplet)):\n",
    "        if (one_couplet[i] >= 3):    # ignore pad,sos,eos\n",
    "            o = o + int2char_dict.get(one_couplet[i].item())   # one_couplet should be like [24,576,134]\n",
    "    return o\n",
    "    \n",
    "\n",
    "def detokenize_II(one_couplet, int2char_dict,n):\n",
    "    o = ''\n",
    "    n = min(len(one_couplet),n)\n",
    "    for i in range(0, n):\n",
    "        if (couplet[i] >= 3):    # ignore pad,sos,eos\n",
    "            o = o + int2char_dict.get(one_couplet[i].item())  \n",
    "\n",
    "    return o\n",
    "    \n",
    "def detokenize_All(couplets, int2char_dict):\n",
    "    o = ''\n",
    "    for i in range(0, len(couplets)):\n",
    "        for j in range(len(couplets[i])):\n",
    "            if (couplets[i][j] >= 3):    # ignore pad,sos,eos\n",
    "                o = o + int2char_dict.get(couplets[i][j].item()) \n",
    "        o = o +\"\\n\"\n",
    "    return o\n",
    "\n",
    "\n",
    "\n",
    "# create a dictionary of repeted words\n",
    "def dict_repet_idx(one_tensor,length):\n",
    "    records_array = np.array(one_tensor)[:length]\n",
    "    vals, inverse, count = np.unique(records_array, return_inverse=True,return_counts=True)\n",
    "    idx_vals_repeated = np.where(count > 1)[0]\n",
    "    vals_repeated = vals[idx_vals_repeated]\n",
    "\n",
    "    rows, cols = np.where(inverse == idx_vals_repeated[:, np.newaxis])\n",
    "    _, inverse_rows = np.unique(rows, return_index=True)\n",
    "    res = np.split(cols, inverse_rows[1:])\n",
    "    \n",
    "    d = dict()\n",
    "    for i in range(len(res)):\n",
    "        for l in range(1,len(res[i])):\n",
    "            val = res[i][0]\n",
    "            d[ res[i][l] ]  = val\n",
    "    \n",
    "    return d\n",
    "\n",
    "\n",
    "# save locally\n",
    "def write_to(path,words):\n",
    "    f = open(path,\"w\")\n",
    "    f.write(words)\n",
    "    f.write('\\n')\n",
    "    f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "#####humain start#########\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(in_path,out_path,with_all_couplets=True):\n",
    "    batch_size = 1 # ------------\n",
    "    nb_couplets = 1024\n",
    "    \n",
    "    print(\"### data preparing ### \")\n",
    "    print(\"Batch size is: \",batch_size)\n",
    "    print(\"Use all couplets to train: \",with_all_couplets)\n",
    "    if (with_all_couplets==False):\n",
    "        print(\"\\tTherefore nb_couplets to train is: \",nb_couplets)\n",
    "\n",
    "    print(\"### data loading ### \")\n",
    "    vocab_path = \"./vocab_total_ori.txt\"\n",
    "    data_in, data_out, max_length = load_data(in_path,out_path,nb_couplets,with_all_couplets)\n",
    "    vocab = load_vocab(vocab_path)\n",
    "    vocab_size = len(vocab)\n",
    "    print(\"Vocab length is \", vocab_size)\n",
    "    return data_in,data_out,vocab,max_length\n",
    "\n",
    "def create_dicts(data_in,data_out,vocab,max_length):\n",
    "\n",
    "    # create char2int and int2char_dict\n",
    "    char2int_dict, int2char_dict = create_char2int_and_int2char_dict(vocab)\n",
    "    \n",
    "    char2tone_dict = create_char2tone_dict(vocab)\n",
    "    \n",
    "    int2tone_dict = dict()\n",
    "    for i in range(len(vocab)):\n",
    "        t = char2tone_dict.get(vocab[i])\n",
    "        int2tone_dict[i] = t\n",
    "\n",
    "    return char2int_dict, int2char_dict, char2tone_dict, int2tone_dict\n",
    "        \n",
    "        \n",
    "def create_tone_mask(vocab, char2tone_dict):\n",
    "    tone_onlyP_2 = np.ones(len(vocab))\n",
    "    tone_onlyZ_2 = np.ones(len(vocab))\n",
    "    for i in range(len(vocab)):\n",
    "        # i  <=> char_id   \n",
    "        tone = char2tone_dict.get(vocab[i])\n",
    "        if (tone.isalpha()):\n",
    "            if (len(tone) != 2):\n",
    "                if(tone == 'P'):\n",
    "                    tone_onlyP_2[i] = 92233720368547758\n",
    "                else:\n",
    "                    tone_onlyZ_2[i] = 92233720368547758\n",
    "    return tone_onlyP_2, tone_onlyZ_2\n",
    "\n",
    "def tokenization(data_in,data_out,char2int_dict,max_length):\n",
    "    # Line One from character to int\n",
    "    Enc_in =  tokenize(data_in,char2int_dict,max_length)\n",
    "    # Line two from character to int, with SOS and EOS\n",
    "    token_out =tokenize(data_out,char2int_dict,max_length)\n",
    "    Dec_in = tokenize_dec_in(data_out,char2int_dict,max_length)\n",
    "    Dec_out = tokenize_dec_out(data_out,char2int_dict,max_length)  \n",
    "    \n",
    "    return  Enc_in,Dec_in,Dec_out\n",
    "\n",
    "def prepare_batches(Enc_in,Dec_out,Dec_in,batch_size=1):\n",
    "    in_ = []\n",
    "    for i in range(0,int(len(Enc_in)/batch_size)):\n",
    "        in_.append( torch.Tensor(Enc_in[batch_size*i:batch_size*(i+1)]).to(torch.int64) )\n",
    "    out_ = []\n",
    "    for i in range(0,int(len(Dec_out)/batch_size)):\n",
    "        out_.append( torch.Tensor(Dec_out[batch_size*i:batch_size*(i+1)]).to(torch.int64) )\n",
    "    dec_in_ =[]\n",
    "    for i in range(0,int(len(Dec_in)/batch_size)):\n",
    "        dec_in_.append( torch.Tensor(Dec_in[batch_size*i:batch_size*(i+1)]).to(torch.int64) )\n",
    "    batch_in_out_pairs =[]   # may ignore some last couplets\n",
    "    for i in range(0, len(in_)):\n",
    "        batch_in_out_pairs.append((in_[i],dec_in_[i], out_[i]))\n",
    "    print(\"Number of couplet batches :\",len(batch_in_out_pairs))\n",
    "    \n",
    "    return batch_in_out_pairs\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "class Node:\n",
    "    def __init__(self,indices,acc,hid,attn):\n",
    "        self.indices = indices\n",
    "        self.acc = acc\n",
    "        self.hid = hid\n",
    "        self.attn = attn\n",
    "\n",
    "        \n",
    "    def add_vi(self,val_,idx_,hid,attn):\n",
    "        self.indices.append(idx_)\n",
    "        self.acc = val_\n",
    "        self.hid.append(hid)\n",
    "        self.attn.append(attn)\n",
    "    \n",
    "        \n",
    "    \n",
    "    def printf(self):\n",
    "        print(\"=> In Node:\")\n",
    "        print(\"=> Acc is: \",float(self.acc))\n",
    "        print(\"=> Indices are:\", [i for i in self.indices])\n",
    "\n",
    "def detokenize_All(couplets, int2char_dict):\n",
    "    o = ''\n",
    "    for i in range(0, len(couplets)):\n",
    "        for j in range(len(couplets[i])):\n",
    "            if (couplets[i][j] >= 3):    # ignore pad,sos,eos\n",
    "                o = o + int2char_dict.get(couplets[i][j].item()) \n",
    "        #o = o +\"\\n\"\n",
    "    return o\n",
    "\n",
    "def detokenize_int(one_couplet, int2char_dict):\n",
    "    o = ''\n",
    "    for i in range(0, len(one_couplet)):\n",
    "        if(int2char_dict.get(one_couplet[i]) is not None):\n",
    "            o = o + int2char_dict.get(one_couplet[i])   # one_couplet should be like [24,576,134]\n",
    "        else:\n",
    "            print(\"what\")\n",
    "    return o\n",
    "\n",
    "\n",
    "# create a dictionary of repeted words\n",
    "def dict_repet_idx(one_tensor,length):\n",
    "    records_array = np.array(one_tensor)[:length]\n",
    "    vals, inverse, count = np.unique(records_array, return_inverse=True,return_counts=True)\n",
    "    idx_vals_repeated = np.where(count > 1)[0]\n",
    "    vals_repeated = vals[idx_vals_repeated]\n",
    "\n",
    "    rows, cols = np.where(inverse == idx_vals_repeated[:, np.newaxis])\n",
    "    _, inverse_rows = np.unique(rows, return_index=True)\n",
    "    res = np.split(cols, inverse_rows[1:])\n",
    "    \n",
    "    d = dict()\n",
    "    for i in range(len(res)):\n",
    "        for l in range(1,len(res[i])):\n",
    "            val = res[i][0]\n",
    "            d[ res[i][l] ]  = val\n",
    "    \n",
    "    return d\n",
    "\n",
    "\n",
    "\n",
    "def detokenize_int_repet(input_tensor, one_couplet, int2char_dict):\n",
    "    o = ''\n",
    "    longueur = np.where(np.array(input_tensor[0]) == 0)[0][0]\n",
    "    d = dict_repet_idx(input_tensor[0],longueur)\n",
    "    #print(\"Input length is \",longueur)\n",
    "    \n",
    "    for i in range(0, len(one_couplet)):\n",
    "        idx = d.get(i)\n",
    "        if ( idx is not None ):\n",
    "            # use the first appreared word\n",
    "            o = o + int2char_dict.get(one_couplet[idx])\n",
    "        else:\n",
    "            o = o + int2char_dict.get(one_couplet[i])\n",
    "    return o\n",
    "\n",
    "def model_in(i,j):\n",
    "    l=[]\n",
    "    l.append(Enc_in[i])\n",
    "    #l.append(Enc_in[i])\n",
    "    #l.append(Enc_in[j])\n",
    "    #l.append(Enc_in[j])\n",
    "    #l.append(Enc_in[j])\n",
    "    l = torch.Tensor(l).to(torch.int64)\n",
    "    q=[]\n",
    "    q.append(Dec_in[i])\n",
    "    #q.append(Dec_in[i])\n",
    "    #q.append(Dec_in[j])\n",
    "    #q.append(Dec_in[j])\n",
    "    #q.append(Dec_in[j])\n",
    "    q = torch.Tensor(q).to(torch.int64)\n",
    "    return l,q\n",
    "\n",
    "\n",
    "### evaluate one couplet\n",
    "def evaluate(char2tone_dict,encoder, decoder,input_tensor,forced_tone=True,forced_word=True, beam_width=1, max_length=33):\n",
    "    \n",
    "    batch_size = input_tensor.size(0)\n",
    "    target_length = input_tensor.size(1)\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    longueur = [np.where(np.array(input_tensor[0]) == 0)[0][0]]\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden = torch.zeros(2, batch_size, hidden_size)\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden,longueur)\n",
    "        decoder_input = torch.tensor([1])\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input.to(device), decoder_hidden.to(device), encoder_outputs.to(device))\n",
    "        \n",
    "        if(forced_tone):\n",
    "            tone = int2tone_dict.get(int(input_tensor[0][0]))\n",
    "            if( tone is not None and tone.isalpha() and len(tone) != 2 ):\n",
    "\n",
    "                if (tone == 'P'):\n",
    "                    decoder_output = decoder_output*torch.Tensor(tone_onlyP_2)\n",
    "                else:\n",
    "                    decoder_output = decoder_output*torch.Tensor(tone_onlyZ_2)        \n",
    "        \n",
    "        \n",
    "        topv, topi = decoder_output.squeeze(1).topk(beam_width)\n",
    "        \n",
    "        decoder_input = topi.detach().t() # t() for convenience dec_in shape\n",
    "        \n",
    "        list_nodes = []\n",
    "        init_node = Node( [], 0,[],[])  # indices,acc,hid,attn\n",
    "        for l in range(0,beam_width):\n",
    "            node = deepcopy(init_node)\n",
    "            node.add_vi(topv[0][l],int(topi[0][l]),decoder_hidden,decoder_attention)\n",
    "            list_nodes.append(node)  \n",
    "\n",
    "        for j in range (1,longueur[0]):\n",
    "            temp_val = []\n",
    "            temp_idx = []\n",
    "            temp_hid = []\n",
    "            temp_attn = []\n",
    "            for b in range(beam_width):\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(torch.tensor([ list_nodes[b].indices[-1] ]), list_nodes[b].hid[-1], encoder_outputs)\n",
    "                \n",
    "                if(forced_tone):\n",
    "                    tone = int2tone_dict.get(int(input_tensor[0][j]))\n",
    "                    if( tone is not None and tone.isalpha() and len(tone) != 2 ):\n",
    "\n",
    "                        if (tone == 'P'):\n",
    "                            decoder_output = decoder_output*torch.Tensor(tone_onlyP_2)  \n",
    "                        else:\n",
    "                            decoder_output = decoder_output*torch.Tensor(tone_onlyZ_2) \n",
    "                            \n",
    "                \n",
    "                topv, topi = decoder_output.squeeze(1).topk(beam_width)\n",
    "                \n",
    "                temp_val.append(topv) \n",
    "                temp_idx.append(topi)\n",
    "                temp_hid.append(decoder_hidden)\n",
    "                temp_attn.append(decoder_attention)\n",
    "                \n",
    "            for bb in range(beam_width):\n",
    "                temp_val[bb] += list_nodes[bb].acc \n",
    "            \n",
    "            concat = temp_val[0]\n",
    "            concat_idx = temp_idx[0]\n",
    "            concat_hid = []\n",
    "            concat_hid.append(temp_hid[0])\n",
    "            concat_attn = []\n",
    "            concat_attn.append(temp_attn[0])\n",
    "            for bbb in range(1,beam_width):\n",
    "                concat = torch.cat((concat,temp_val[bbb]),dim=1)\n",
    "                concat_idx = torch.cat((concat_idx,temp_idx[bbb]),dim=1)\n",
    "                concat_hid.append(temp_hid[bbb])\n",
    "                concat_attn.append(temp_attn[bbb])\n",
    "            concat = concat.squeeze()\n",
    "            concat_idx = concat_idx.squeeze()\n",
    "            if(beam_width == 1):\n",
    "                concat = concat.unsqueeze(0)\n",
    "                concat_idx = concat_idx.unsqueeze(0)\n",
    "\n",
    "            tv, ti = concat.topk(beam_width)\n",
    "\n",
    "            new_list_nodes = []\n",
    "            for bbbb in range(beam_width):\n",
    "                quel_node =  int(int(ti[bbbb]) / beam_width)\n",
    "                corr_nodes = list_nodes[quel_node] \n",
    "\n",
    "                new = deepcopy(corr_nodes)\n",
    "                new.add_vi(concat[ti[bbbb]], int(concat_idx[ti[bbbb]]),concat_hid[quel_node],concat_attn[quel_node] )\n",
    "\n",
    "                new_list_nodes.append(deepcopy(new))\n",
    "\n",
    "            new_dec_in = []\n",
    "            for e in range(len(new_list_nodes)):\n",
    "                nn = new_list_nodes[e]\n",
    "                new_dec_in.append(nn.indices[-1])\n",
    "            new_dec_in = torch.tensor(new_dec_in).unsqueeze(1)\n",
    "            \n",
    "            list_nodes = deepcopy(new_list_nodes)\n",
    "            decoder_input = new_dec_in\n",
    "\n",
    "        decoded_words=[]\n",
    "        for kk in range(len(list_nodes)):\n",
    "            nnn = list_nodes[kk]\n",
    "            d = nnn.indices\n",
    "            \n",
    "            if (forced_word):\n",
    "                decoded_words.append( detokenize_int_repet(input_tensor, d, int2char_dict) )\n",
    "            else:\n",
    "                decoded_words.append( detokenize_int(d, int2char_dict) )\n",
    "    return decoded_words, list_nodes\n",
    "            \n",
    "# evaluate randomly n couplets\n",
    "def evaluateRandomly(char2tone_dict,encoder, decoder,forced_tone=True,forced_word=False,beam_width=1, n=1):\n",
    "\n",
    "    for i in range(n):\n",
    "        \n",
    "        i = np.random.randint(len(Enc_in))\n",
    "        \n",
    "        j = np.random.randint(len(Enc_in))\n",
    "        pair, d_out = model_in(i,j)\n",
    "        output_words, list_nodes = evaluate(char2tone_dict,encoder, decoder, pair,forced_tone,forced_word,beam_width)\n",
    "        output_words_line = '\\n'.join(output_words)\n",
    "        \n",
    "        print(\"in: \")\n",
    "        inn = detokenize_All(pair, int2char_dict)\n",
    "        print(inn)\n",
    "        print('model answer: ')\n",
    "        print(output_words_line)\n",
    "        print(\"desired answer: \")\n",
    "        outt = detokenize_All(d_out, int2char_dict)\n",
    "        print(outt)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        \n",
    "        # If you want to plot the attention\n",
    "        # Decomment the following code\n",
    "        #\"\"\"\"\"\n",
    "        attn = []\n",
    "        for w in range(len(list_nodes)):\n",
    "            #print(\"w is \",w)\n",
    "            a = list_nodes[w].attn\n",
    "            #print(\"len attn\",len(a))\n",
    "            attn_tmp = torch.tensor(())\n",
    "            for m in range(len(a)):\n",
    "                b = torch.tensor(a[m][0])\n",
    "                #plt.subplots(figsize=(5, 9))\n",
    "                #plt.imshow(b,cmap='hot')\n",
    "                #plt.show()\n",
    "                attn_tmp = torch.cat((attn_tmp, b),0) \n",
    "                \n",
    "            #showAttention(pair, output_words[k], list_nodes[k].attn)\n",
    "            #print(\"tmp shape\",attn_tmp.shape)  # for j in longueur or max_length\n",
    "            \n",
    "            plt.subplots(figsize=(3, 2))\n",
    "            #plt.imshow(attn_tmp[:8,:8],cmap='hot')\n",
    "            plt.imshow(attn_tmp,cmap='hot')\n",
    "\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "            attn.append(attn_tmp)\n",
    "        #\"\"\"\"\"\n",
    "    \n",
    "### bleu and attn ###\n",
    "def compute_bleu(char2tone_dict,encoder, decoder,forced_tone=False,forced_word=False,beam_width=1, n=1):\n",
    "    total_bleu = 0\n",
    "    count=0\n",
    "    #for i in range(len(batch_in_out_pairs_eval)):\n",
    "    for i in range(n):#len(batch_in_out_pairs_eval)):\n",
    "        \n",
    "        if ( i % int(0.1*n) == 0):\n",
    "            print( i / n)\n",
    "        #print(\"idx is \",i)\n",
    "        \n",
    "        j = np.random.randint(len(Enc_in))\n",
    "        pair, d_out = model_in(i,j)\n",
    "        output_words, list_nodes = evaluate(char2tone_dict,encoder, decoder, pair,forced_tone,forced_word,beam_width)\n",
    "        output_words_line = '\\n'.join(output_words)\n",
    "        inn = detokenize_All(pair, int2char_dict)\n",
    "        outt = detokenize_All(d_out, int2char_dict)\n",
    "       \n",
    "        for k in range(len(output_words)):\n",
    "            smoothie = SmoothingFunction().method4\n",
    "            if( len(outt) == 1 and outt==output_words[k]):  # otherwise division 0\n",
    "                total_bleu += 1.0\n",
    "                count+=1\n",
    "            else:\n",
    "                b = bleu([outt], output_words[k], smoothing_function=smoothie)\n",
    "                total_bleu += b\n",
    "                if(b >= 0.2):\n",
    "                    count+=1\n",
    "                    \n",
    "    print(\"## FIN ##\")\n",
    "    print(\"BLEU :\",total_bleu/ (n*beam_width)  )\n",
    "    print(\"#count >0.2 :\",count/beam_width)        \n",
    "        \n",
    "# need data_in, data_out, Enc_in Dec_out\n",
    "def for_humain_eval(i,n,pt,char2tone_dict,indices,om,forced_tone=True,forced_word=False,beam_width=1):\n",
    "    encoder = torch.load(\"./models/II_enc_epoch_\"+pt+\".pt\",map_location='cpu')\n",
    "    attn_decoder = torch.load(\"./models/II_dec_epoch_\"+pt+\".pt\",map_location='cpu')\n",
    "    encoder.eval()\n",
    "    attn_decoder.eval()\n",
    "    print(\"encoder.hidden_size:\",encoder.hidden_size)\n",
    "    print(\"attn_decoder.hidden_size:\",attn_decoder.hidden_size)\n",
    "\n",
    "    f = open(\"./random_\"+str(i)+\"_\"+pt+\"_\"+str(forced_tone)+\"_\"+str(forced_word)+\"_\"+str(beam_width)+\".txt\",\"w\")\n",
    "    f_oomm = open(\"./random_oomm_\"+str(i)+\"_\"+pt+\"_\"+str(forced_tone)+\"_\"+str(forced_word)+\"_\"+str(beam_width)+\".txt\",\"w\")\n",
    "    f_a = open(\"./random_notes_\"+str(i)+\"_\"+pt+\"_\"+str(forced_tone)+\"_\"+str(forced_word)+\"_\"+str(beam_width)+\".txt\",\"w\")\n",
    "    \n",
    "    \n",
    "    f_a.write(\"### oo => 0, mm => 1 ###\")\n",
    "    f_a.write(\"\\n\")\n",
    "    \n",
    "    for idx in range(n):\n",
    "        \n",
    "        f_a.write(\"-Question-\"+str(idx+1)+\"\\n\")\n",
    "        f_a.write(\"Indices:\"+str(indices[idx])+\"\\n\")\n",
    "        f_a.write(\"Truth:\"+str(om[idx])+\"\\n\")\n",
    "        f_a.write(\"\\n\")\n",
    "    \n",
    "        # generate model answer\n",
    "        i = indices[idx]\n",
    "        pair = torch.Tensor(Enc_in[i]).to(torch.int64).unsqueeze(0)\n",
    "        d_out = torch.Tensor(Dec_in[i]).to(torch.int64).unsqueeze(0)\n",
    "        output_words, _ = evaluate(char2tone_dict,encoder, attn_decoder, pair,forced_tone,forced_word,beam_width)\n",
    "        \n",
    "        # without oomm \n",
    "        f.write(\"-Question-\"+str(idx+1)+\"\\n\")\n",
    "        f.write(\"Indices:\"+str(i)+\"\\n\")\n",
    "        f.write(data_in[i]+\" ii\\n\")\n",
    "        if(om[idx] == 0):\n",
    "            f.write(data_out[i]+\"\\n\")\n",
    "            f.write(output_words[0]+\"\\n\")\n",
    "        else:\n",
    "            f.write(output_words[0]+\"\\n\")\n",
    "            f.write(data_out[i]+\"\\n\")\n",
    "        f.write('\\n')\n",
    "        \n",
    "        # with oomm    \n",
    "        f_oomm.write(\"-Question-\"+str(idx+1)+\"\\n\")\n",
    "        f_oomm.write(\"Indices:\"+str(i)+\"\\n\")\n",
    "        f_oomm.write(data_in[i]+\" ii\\n\")   \n",
    "        if(om[idx] == 0): \n",
    "            f_oomm.write(data_out[i]+\" oo\\n\")\n",
    "            f_oomm.write(output_words[0]+\" mm\\n\")\n",
    "        else:\n",
    "            f_oomm.write(output_words[0]+\" mm\\n\")\n",
    "            f_oomm.write(data_out[i]+\" oo\\n\")            \n",
    "        f_oomm.write('\\n')\n",
    "        \n",
    "    f_a.close\n",
    "    f.close\n",
    "    f_oomm.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_path = \"./test_in.txt\"\n",
    "out_path = \"./test_out.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### data preparing ### \n",
      "Batch size is:  1\n",
      "Use all couplets to train:  True\n",
      "### data loading ### \n",
      "min line length is  1\n",
      "max line length is  32\n",
      "Loaded input couplets length is  3931\n",
      "Loaded output couplets length is  3931\n",
      "Vocab length is  9131\n",
      "Number of couplet batches : 3931\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "data_in,data_out,vocab,max_length = prepare_data(in_path,out_path,with_all_couplets=True)\n",
    "char2int_dict, int2char_dict, char2tone_dict, int2tone_dict = create_dicts(data_in,data_out,vocab,max_length)\n",
    "tone_onlyP_2, tone_onlyZ_2 = create_tone_mask(vocab, char2tone_dict)\n",
    "Enc_in,Dec_in,Dec_out = tokenization(data_in,data_out,char2int_dict,max_length)\n",
    "batch_in_out_pairs_eval = prepare_batches(Enc_in,Dec_out,Dec_in,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.hidden_size: 256\n",
      "attn_decoder.hidden_size: 256\n",
      "encoder.hidden_size: 256\n",
      "attn_decoder.hidden_size: 256\n",
      "encoder.hidden_size: 256\n",
      "attn_decoder.hidden_size: 256\n",
      "encoder.hidden_size: 256\n",
      "attn_decoder.hidden_size: 256\n",
      "encoder.hidden_size: 256\n",
      "attn_decoder.hidden_size: 256\n",
      "encoder.hidden_size: 256\n",
      "attn_decoder.hidden_size: 256\n",
      "encoder.hidden_size: 256\n",
      "attn_decoder.hidden_size: 256\n",
      "encoder.hidden_size: 256\n",
      "attn_decoder.hidden_size: 256\n",
      "encoder.hidden_size: 256\n",
      "attn_decoder.hidden_size: 256\n",
      "encoder.hidden_size: 256\n",
      "attn_decoder.hidden_size: 256\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,20):\n",
    "    indices = np.random.choice(len(Enc_in), n, replace = False)\n",
    "    om = np.random.choice(2, n, replace = True)\n",
    "    for_humain_eval(i,n,\"203\",char2tone_dict,indices,om,forced_tone=True,forced_word=True,beam_width=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = torch.load(\"./models/II_enc_epoch_203.pt\",map_location='cpu')\n",
    "attn_decoder = torch.load(\"./models/II_dec_epoch_203.pt\",map_location='cpu')\n",
    "encoder.eval()\n",
    "attn_decoder.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate randomly n couplets\n",
    "def evaluateRandomly(char2tone_dict,encoder, decoder,forced_tone=True,forced_word=False,beam_width=1, n=1):\n",
    "    f = open(\"./cou_attn.txt\",\"a\")\n",
    "    for i in range(n):\n",
    "        \n",
    "        i = np.random.randint(len(Enc_in))\n",
    "        \n",
    "        j = np.random.randint(len(Enc_in))\n",
    "        pair, d_out = model_in(i,j)\n",
    "        output_words, list_nodes = evaluate(char2tone_dict,encoder, decoder, pair,forced_tone,forced_word,beam_width)\n",
    "        output_words_line = '\\n'.join(output_words)\n",
    "        f.write(str(i))\n",
    "        f.write(\"\\n\")\n",
    "        print(\"in: \")\n",
    "        inn = detokenize_All(pair, int2char_dict)\n",
    "        print(inn)\n",
    "        f.write(inn)\n",
    "        f.write(\"\\n\")\n",
    "        print('model answer: ')\n",
    "        print(output_words_line)\n",
    "        f.write(output_words_line)\n",
    "        f.write(\"\\n\")\n",
    "        print(\"desired answer: \")\n",
    "        outt = detokenize_All(d_out, int2char_dict)\n",
    "        print(outt)\n",
    "        f.write(outt)\n",
    "        f.write(\"\\n\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        \n",
    "        # If you want to plot the attention\n",
    "        # Decomment the following code\n",
    "        #\"\"\"\"\"\n",
    "        attn = []\n",
    "        for w in range(len(list_nodes)):\n",
    "            #print(\"w is \",w)\n",
    "            a = list_nodes[w].attn\n",
    "            #print(\"len attn\",len(a))\n",
    "            attn_tmp = torch.tensor(())\n",
    "            for m in range(len(a)):\n",
    "                b = torch.tensor(a[m][0])\n",
    "                #plt.subplots(figsize=(5, 9))\n",
    "                #plt.imshow(b,cmap='hot')\n",
    "                #plt.show()\n",
    "                attn_tmp = torch.cat((attn_tmp, b),0) \n",
    "                \n",
    "            #showAttention(pair, output_words[k], list_nodes[k].attn)\n",
    "            #print(\"tmp shape\",attn_tmp.shape)  # for j in longueur or max_length\n",
    "            \n",
    "            plt.subplots(figsize=(5, 9))\n",
    "            #plt.imshow(attn_tmp[:8,:8],cmap='hot')\n",
    "            plt.imshow(attn_tmp,cmap='hot')\n",
    "            \n",
    "            plt.savefig('cou_'+str(i)+'.png')\n",
    "\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "            attn.append(attn_tmp)\n",
    "        #\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: \n",
      "古道雄关，翠柏苍松芳草绿\n",
      "model answer: \n",
      "清风明月，清风明月玉箫清\n",
      "desired answer: \n",
      "英风浩气，丹心碧血杜鹃红\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/x/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluateRandomly(char2tone_dict,encoder, attn_decoder,forced_tone=True,forced_word=True,beam_width=1,n=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "## FIN ##\n",
      "BLEU : 0.17173033244710195\n",
      "#count >0.2 : 4.0\n"
     ]
    }
   ],
   "source": [
    "# 203\n",
    "compute_bleu(char2tone_dict,encoder, attn_decoder,forced_tone=True,forced_word=True,beam_width=1,n=len(batch_in_out_pairs_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5be53cce3582>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 203\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcompute_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar2tone_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_decoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforced_tone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforced_word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeam_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_in_out_pairs_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-8ffcc737032e>\u001b[0m in \u001b[0;36mcompute_bleu\u001b[0;34m(char2tone_dict, encoder, decoder, forced_tone, forced_word, beam_width, n)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEnc_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mpair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0moutput_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar2tone_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforced_tone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforced_word\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeam_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0moutput_words_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0minn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetokenize_All\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint2char_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-8ffcc737032e>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(char2tone_dict, encoder, decoder, input_tensor, forced_tone, forced_word, beam_width, max_length)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mdecoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforced_tone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "# 203\n",
    "compute_bleu(char2tone_dict,encoder, attn_decoder,forced_tone=True,forced_word=True,beam_width=3,n=len(batch_in_out_pairs_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.09997456118036123\n",
      "0.19994912236072246\n",
      "0.2999236835410837\n",
      "0.3998982447214449\n",
      "0.49987280590180616\n",
      "0.5998473670821673\n",
      "0.6998219282625286\n",
      "0.7997964894428898\n",
      "0.8997710506232511\n",
      "0.9997456118036123\n",
      "## FIN ##\n",
      "BLEU : 0.13732333521721898\n",
      "#count >0.2 : 1146.6\n"
     ]
    }
   ],
   "source": [
    "# 203\n",
    "compute_bleu(char2tone_dict,encoder, attn_decoder,forced_tone=True,forced_word=True,beam_width=5,n=len(batch_in_out_pairs_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
