{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from pypinyin import pinyin, lazy_pinyin, Style\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from nltk.translate import bleu\n",
    "from copy import deepcopy\n",
    "\n",
    "from utils import *\n",
    "# Define a device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(in_path,out_path,with_all_couplets=True):\n",
    "    batch_size = 1 \n",
    "    nb_couplets = 1024\n",
    "    \n",
    "    print(\"### data preparing ### \")\n",
    "    print(\"Batch size is: \",batch_size)\n",
    "    print(\"Use all couplets to train: \",with_all_couplets)\n",
    "    if (with_all_couplets==False):\n",
    "        print(\"\\tTherefore nb_couplets to train is: \",nb_couplets)\n",
    "\n",
    "    print(\"### data loading ### \")\n",
    "    vocab_path = \"./vocab.txt\"\n",
    "    data_in, data_out, max_length = load_data(in_path,out_path,nb_couplets,with_all_couplets)\n",
    "    vocab = load_vocab(vocab_path)\n",
    "    vocab_size = len(vocab)\n",
    "    print(\"Vocab length is \", vocab_size)\n",
    "    return data_in,data_out,vocab,max_length\n",
    "\n",
    "def create_dicts(data_in,data_out,vocab,max_length):\n",
    "\n",
    "    # create char2int and int2char_dict\n",
    "    char2int_dict, int2char_dict = create_char2int_and_int2char_dict(vocab)\n",
    "    \n",
    "    char2tone_dict = create_char2tone_dict(vocab)\n",
    "    \n",
    "    int2tone_dict = dict()\n",
    "    for i in range(len(vocab)):\n",
    "        t = char2tone_dict.get(vocab[i])\n",
    "        int2tone_dict[i] = t\n",
    "\n",
    "    return char2int_dict, int2char_dict, char2tone_dict, int2tone_dict\n",
    "        \n",
    "        \n",
    "def create_tone_mask(vocab, char2tone_dict):\n",
    "    tone_onlyP_2 = np.ones(len(vocab))\n",
    "    tone_onlyZ_2 = np.ones(len(vocab))\n",
    "    tone_onlyZ_2[:4] = 92233720368547758  # remove PAD, SOS,EOS and UNK\n",
    "    tone_onlyZ_2[:4] = 92233720368547758\n",
    "    for i in range(4,len(vocab)):\n",
    "        # i  <=> char_id   \n",
    "        tone = char2tone_dict.get(vocab[i])\n",
    "        if (tone.isalpha()):\n",
    "            if (len(tone) != 2):\n",
    "                if(tone == 'P'):\n",
    "                    tone_onlyP_2[i] = 92233720368547758\n",
    "                else:\n",
    "                    tone_onlyZ_2[i] = 92233720368547758\n",
    "    return tone_onlyP_2, tone_onlyZ_2\n",
    "\n",
    "    \n",
    "# tokenize: couplet to int \n",
    "def tokenize_(data, char2int_dict, max_length):  \n",
    "    v=[]\n",
    "    for i in range(0, len(data)):\n",
    "        l = []\n",
    "        for j in range (0, len(data[i])):\n",
    "            if ( char2int_dict.get(data[i][j])  is None ):\n",
    "                l.append(   char2int_dict.get('UNK')  )\n",
    "            else:\n",
    "                l.append(   char2int_dict.get(data[i][j])  )\n",
    "        if (len(data[i]) < max_length):   # padding 0(pad) at the end\n",
    "            l = (l + [0] * max_length)[:max_length]\n",
    "        v.append(l)\n",
    "    return v\n",
    "\n",
    "\n",
    "def tokenize_dec_in_(data, char2int_dict, max_length): \n",
    "    v=[]\n",
    "    for i in range(0, len(data)):\n",
    "        l = [char2int_dict.get('sos')]\n",
    "\n",
    "        for j in range (0, len(data[i])):\n",
    "            if ( char2int_dict.get(data[i][j])  is None ):\n",
    "                l.append(   char2int_dict.get('UNK')  )\n",
    "            else:\n",
    "                l.append(   char2int_dict.get(data[i][j])  )\n",
    "        if (len(data[i]) < max_length):   # padding 0(pad) at the end\n",
    "            l = (l + [0] * max_length)[:max_length]\n",
    "        v.append(l)\n",
    "    return v\n",
    "\n",
    "\n",
    "def tokenize_dec_out_(data, char2int_dict, max_length): \n",
    "    v=[]\n",
    "    for i in range(0, len(data)):\n",
    "        l = []\n",
    "        for j in range (0, len(data[i])):\n",
    "            if ( char2int_dict.get(data[i][j])  is None ):\n",
    "                l.append(   char2int_dict.get('UNK')  )\n",
    "            else:\n",
    "                l.append(   char2int_dict.get(data[i][j])  )\n",
    "\n",
    "        l.append(char2int_dict.get('eos'))\n",
    "        if (len(data[i]) < max_length):   # padding 0(pad) at the end\n",
    "            l = (l + [0] * max_length)[:max_length]\n",
    "        v.append(l)\n",
    "    return v\n",
    "\n",
    "def tokenization(data_in,data_out,char2int_dict,max_length):\n",
    "    # Line One from character to int\n",
    "    Enc_in =  tokenize_(data_in,char2int_dict,max_length)\n",
    "    # Line two from character to int, with SOS and EOS\n",
    "    token_out =tokenize_(data_out,char2int_dict,max_length)\n",
    "    Dec_in = tokenize_dec_in_(data_out,char2int_dict,max_length)\n",
    "    Dec_out = tokenize_dec_out_(data_out,char2int_dict,max_length)  \n",
    "    \n",
    "    return  Enc_in,Dec_in,Dec_out\n",
    "\n",
    "def prepare_batches(Enc_in,Dec_out,Dec_in,batch_size=1):\n",
    "    in_ = []\n",
    "    for i in range(0,int(len(Enc_in)/batch_size)):\n",
    "        in_.append( torch.Tensor(Enc_in[batch_size*i:batch_size*(i+1)]).to(torch.int64) )\n",
    "    out_ = []\n",
    "    for i in range(0,int(len(Dec_out)/batch_size)):\n",
    "        out_.append( torch.Tensor(Dec_out[batch_size*i:batch_size*(i+1)]).to(torch.int64) )\n",
    "    dec_in_ =[]\n",
    "    for i in range(0,int(len(Dec_in)/batch_size)):\n",
    "        dec_in_.append( torch.Tensor(Dec_in[batch_size*i:batch_size*(i+1)]).to(torch.int64) )\n",
    "    batch_in_out_pairs =[]   # may ignore some last couplets\n",
    "    for i in range(0, len(in_)):\n",
    "        batch_in_out_pairs.append((in_[i],dec_in_[i], out_[i]))\n",
    "    print(\"Number of couplet batches :\",len(batch_in_out_pairs))\n",
    "    \n",
    "    return batch_in_out_pairs\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self,indices,acc,hid,attn):\n",
    "        self.indices = indices\n",
    "        self.acc = acc\n",
    "        self.hid = hid\n",
    "        self.attn = attn\n",
    "\n",
    "        \n",
    "    def add_vi(self,val_,idx_,hid,attn):\n",
    "        self.indices.append(idx_)\n",
    "        self.acc = val_\n",
    "        self.hid.append(hid)\n",
    "        self.attn.append(attn)\n",
    "    \n",
    "    def printf(self):\n",
    "        print(\"=> In Node:\")\n",
    "        print(\"=> Acc is: \",float(self.acc))\n",
    "        print(\"=> Indices are:\", [i for i in self.indices])\n",
    "\n",
    "def detokenize_All(couplets, int2char_dict):\n",
    "    o = ''\n",
    "    for i in range(0, len(couplets)):\n",
    "        for j in range(len(couplets[i])):\n",
    "            if (couplets[i][j] >= 3):    # ignore pad,sos,eos\n",
    "                o = o + int2char_dict.get(couplets[i][j].item()) \n",
    "        #o = o +\"\\n\"\n",
    "    return o\n",
    "\n",
    "def detokenize_int(one_couplet, int2char_dict):\n",
    "    o = ''\n",
    "    for i in range(0, len(one_couplet)):\n",
    "        if(int2char_dict.get(one_couplet[i]) is not None):\n",
    "            o = o + int2char_dict.get(one_couplet[i])   # one_couplet should be like [24,576,134]\n",
    "        else:\n",
    "            print(\"what\")\n",
    "    return o\n",
    "\n",
    "\n",
    "# create a dictionary of repeted words\n",
    "def dict_repet_idx(one_tensor,length):\n",
    "    records_array = np.array(one_tensor)[:length]\n",
    "    vals, inverse, count = np.unique(records_array, return_inverse=True,return_counts=True)\n",
    "    idx_vals_repeated = np.where(count > 1)[0]\n",
    "    vals_repeated = vals[idx_vals_repeated]\n",
    "\n",
    "    rows, cols = np.where(inverse == idx_vals_repeated[:, np.newaxis])\n",
    "    _, inverse_rows = np.unique(rows, return_index=True)\n",
    "    res = np.split(cols, inverse_rows[1:])\n",
    "    \n",
    "    d = dict()\n",
    "    for i in range(len(res)):\n",
    "        for l in range(1,len(res[i])):\n",
    "            val = res[i][0]\n",
    "            d[ res[i][l] ]  = val\n",
    "    \n",
    "    return d\n",
    "\n",
    "\n",
    "# If input couplet consists of some same characters\n",
    "# Then this method forces the output couplet to have same character at corresponding position\n",
    "def detokenize_int_repet(input_tensor, one_couplet, int2char_dict):\n",
    "    o = ''\n",
    "    longueur = np.where(np.array(input_tensor[0]) == 0)[0][0]\n",
    "    d = dict_repet_idx(input_tensor[0],longueur)\n",
    "    #print(\"Input length is \",longueur)\n",
    "    \n",
    "    for i in range(0, len(one_couplet)):\n",
    "        idx = d.get(i)\n",
    "        if ( idx is not None ):\n",
    "            # use the first appreared word\n",
    "            o = o + int2char_dict.get(one_couplet[idx])\n",
    "        else:\n",
    "            o = o + int2char_dict.get(one_couplet[i])\n",
    "    return o\n",
    "\n",
    "\n",
    "# Evaluate one couplet\n",
    "def evaluate(char2tone_dict,encoder, decoder,input_tensor,forced_tone=True,forced_word=True, beam_width=1, max_length=33):\n",
    "    \n",
    "    batch_size = input_tensor.size(0)\n",
    "    target_length = input_tensor.size(1)\n",
    "    ini_mask = np.ones(len(vocab))\n",
    "    ini_mask[:4] = 92233720368547758  # masking PAD, SOS, EOS and UNK\n",
    "    ini_mask[input_tensor] = 92233720368547758 # masking characters in inputs\n",
    "    ini_mask[4] = 1\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    longueur = [np.where(np.array(input_tensor[0]) == 0)[0][0]]\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden = torch.zeros(2, batch_size, hidden_size)\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden,longueur)\n",
    "        decoder_input = torch.tensor([1])\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input.to(device), decoder_hidden.to(device), encoder_outputs.to(device))\n",
    "        \n",
    "        decoder_output = decoder_output*torch.Tensor(ini_mask)\n",
    "        \n",
    "        if(forced_tone):\n",
    "            tone = int2tone_dict.get(int(input_tensor[0][0]))\n",
    "            if( tone is not None and tone.isalpha() and len(tone) != 2 ):\n",
    "\n",
    "                if (tone == 'P'):\n",
    "                    decoder_output = decoder_output*torch.Tensor(tone_onlyP_2)\n",
    "                else:\n",
    "                    decoder_output = decoder_output*torch.Tensor(tone_onlyZ_2)        \n",
    "        \n",
    "        \n",
    "        topv, topi = decoder_output.squeeze(1).topk(beam_width)\n",
    "        \n",
    "        decoder_input = topi.detach().t() # t() for convenience dec_in shape\n",
    "        \n",
    "        list_nodes = []\n",
    "        init_node = Node( [], 0,[],[])  # indices,acc,hid,attn\n",
    "        for l in range(0,beam_width):\n",
    "            node = deepcopy(init_node)\n",
    "            node.add_vi(topv[0][l],int(topi[0][l]),decoder_hidden,decoder_attention)\n",
    "            list_nodes.append(node)  \n",
    "\n",
    "        for j in range (1,longueur[0]):\n",
    "            temp_val = []\n",
    "            temp_idx = []\n",
    "            temp_hid = []\n",
    "            temp_attn = []\n",
    "            for b in range(beam_width):\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(torch.tensor([ list_nodes[b].indices[-1] ]), list_nodes[b].hid[-1], encoder_outputs)\n",
    "                \n",
    "                decoder_output = decoder_output*torch.Tensor(ini_mask)\n",
    "                \n",
    "                if(forced_tone):\n",
    "                    tone = int2tone_dict.get(int(input_tensor[0][j]))\n",
    "                    if( tone is not None and tone.isalpha() and len(tone) != 2 ):\n",
    "                        \n",
    "                        if (tone == 'P'):\n",
    "                            decoder_output = decoder_output*torch.Tensor(tone_onlyP_2)  \n",
    "                        else:\n",
    "                            decoder_output = decoder_output*torch.Tensor(tone_onlyZ_2)   \n",
    "                            \n",
    "                \n",
    "                topv, topi = decoder_output.squeeze(1).topk(beam_width)\n",
    "                \n",
    "                temp_val.append(topv) \n",
    "                temp_idx.append(topi)\n",
    "                temp_hid.append(decoder_hidden)\n",
    "                temp_attn.append(decoder_attention)\n",
    "                \n",
    "            for bb in range(beam_width):\n",
    "                temp_val[bb] += list_nodes[bb].acc \n",
    "            \n",
    "            concat = temp_val[0]\n",
    "            concat_idx = temp_idx[0]\n",
    "            concat_hid = []\n",
    "            concat_hid.append(temp_hid[0])\n",
    "            concat_attn = []\n",
    "            concat_attn.append(temp_attn[0])\n",
    "            for bbb in range(1,beam_width):\n",
    "                concat = torch.cat((concat,temp_val[bbb]),dim=1)\n",
    "                concat_idx = torch.cat((concat_idx,temp_idx[bbb]),dim=1)\n",
    "                concat_hid.append(temp_hid[bbb])\n",
    "                concat_attn.append(temp_attn[bbb])\n",
    "            concat = concat.squeeze()\n",
    "            concat_idx = concat_idx.squeeze()\n",
    "            if(beam_width == 1):\n",
    "                concat = concat.unsqueeze(0)\n",
    "                concat_idx = concat_idx.unsqueeze(0)\n",
    "\n",
    "            tv, ti = concat.topk(beam_width)\n",
    "\n",
    "            new_list_nodes = []\n",
    "            for bbbb in range(beam_width):\n",
    "                quel_node =  int(int(ti[bbbb]) / beam_width)\n",
    "                corr_nodes = list_nodes[quel_node] \n",
    "\n",
    "                new = deepcopy(corr_nodes)\n",
    "                new.add_vi(concat[ti[bbbb]], int(concat_idx[ti[bbbb]]),concat_hid[quel_node],concat_attn[quel_node] )\n",
    "\n",
    "                new_list_nodes.append(deepcopy(new))\n",
    "\n",
    "            new_dec_in = []\n",
    "            for e in range(len(new_list_nodes)):\n",
    "                nn = new_list_nodes[e]\n",
    "                new_dec_in.append(nn.indices[-1])\n",
    "            new_dec_in = torch.tensor(new_dec_in).unsqueeze(1)\n",
    "            \n",
    "            list_nodes = deepcopy(new_list_nodes)\n",
    "            decoder_input = new_dec_in\n",
    "\n",
    "        decoded_words=[]\n",
    "        for kk in range(len(list_nodes)):\n",
    "            nnn = list_nodes[kk]\n",
    "            d = nnn.indices\n",
    "            \n",
    "            if (forced_word):\n",
    "                decoded_words.append( detokenize_int_repet(input_tensor, d, int2char_dict) )\n",
    "            else:\n",
    "                decoded_words.append( detokenize_int(d, int2char_dict) )\n",
    "    return decoded_words, list_nodes\n",
    "            \n",
    "    \n",
    "# evaluate randomly n couplets\n",
    "def evaluateRandomly(char2tone_dict,encoder, decoder,forced_tone=True,forced_word=False,beam_width=1, n=1):\n",
    "    # save locally couplet outputs\n",
    "    f = open(\"./couplet_outputs.txt\",\"a\")\n",
    "    for i in range(n):\n",
    "        \n",
    "        i = 2203#np.random.randint(len(Enc_in))\n",
    "        \n",
    "        j = np.random.randint(len(Enc_in))\n",
    "        pair = torch.Tensor([Enc_in[i] ]).to(torch.int64)\n",
    "        d_out =torch.Tensor([Dec_in[i] ]).to(torch.int64)\n",
    "        output_words, list_nodes = evaluate(char2tone_dict,encoder, decoder, pair,forced_tone,forced_word,beam_width)\n",
    "        output_words_line = '\\n'.join(output_words)\n",
    "        f.write(str(i))\n",
    "        f.write(\"\\n\")\n",
    "        print(\"in: \")\n",
    "        inn = detokenize_All(pair, int2char_dict)\n",
    "        print(inn)\n",
    "        f.write(inn)\n",
    "        f.write(\"\\n\")\n",
    "        print('model answer: ')\n",
    "        print(output_words_line)\n",
    "        f.write(output_words_line)\n",
    "        f.write(\"\\n\")\n",
    "        print(\"desired answer: \")\n",
    "        outt = detokenize_All(d_out, int2char_dict)\n",
    "        print(outt)\n",
    "        f.write(outt)\n",
    "        f.write(\"\\n\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        \n",
    "        # Plot the attention and save it locally\n",
    "        attn = []\n",
    "        for w in range(len(list_nodes)):\n",
    "            #print(\"w is \",w)\n",
    "            a = list_nodes[w].attn\n",
    "            #print(\"len attn\",len(a))\n",
    "            attn_tmp = torch.tensor(())\n",
    "            for m in range(len(a)):\n",
    "                b = torch.tensor(a[m][0])\n",
    "                #plt.subplots(figsize=(5, 9))\n",
    "                #plt.imshow(b,cmap='hot')\n",
    "                #plt.show()\n",
    "                attn_tmp = torch.cat((attn_tmp, b),0) \n",
    "                \n",
    "            #showAttention(pair, output_words[k], list_nodes[k].attn)\n",
    "            #print(\"tmp shape\",attn_tmp.shape)  # for j in longueur or max_length\n",
    "            \n",
    "            plt.subplots(figsize=(4, 6))\n",
    "            #plt.imshow(attn_tmp[:8,:8],cmap='hot')\n",
    "            plt.imshow(attn_tmp,cmap='hot')\n",
    "            \n",
    "            plt.savefig('couplet_attn_'+str(i)+'.png')\n",
    "\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "            attn.append(attn_tmp)\n",
    "    \n",
    "# Compute bleu score\n",
    "def compute_bleu(char2tone_dict,encoder, decoder,forced_tone=False,forced_word=False,beam_width=1, n=1):\n",
    "    total_bleu = 0\n",
    "    count=0\n",
    "\n",
    "    for i in range(n):\n",
    "        pair = torch.Tensor([Enc_in[i] ]).to(torch.int64)\n",
    "        d_out =torch.Tensor([Dec_in[i] ]).to(torch.int64)\n",
    "        \n",
    "        output_words, list_nodes = evaluate(char2tone_dict,encoder, decoder, pair,forced_tone,forced_word,beam_width)\n",
    "        output_words_line = '\\n'.join(output_words)\n",
    "        inn = detokenize_All(pair, int2char_dict)\n",
    "        outt = detokenize_All(d_out, int2char_dict)\n",
    "       \n",
    "        for k in range(len(output_words)):\n",
    "            smoothie = SmoothingFunction().method4\n",
    "            if( len(outt) == 1 and outt==output_words[k]):  # otherwise division 0\n",
    "                total_bleu += 1.0\n",
    "                count+=1\n",
    "            else:\n",
    "                b = bleu([outt], output_words[k], smoothing_function=smoothie)\n",
    "                total_bleu += b\n",
    "                if(b >= 0.2):\n",
    "                    count+=1\n",
    "                    \n",
    "    print(\"## FIN ##\")\n",
    "    print(\"BLEU :\",total_bleu/ (n*beam_width)  )\n",
    "    print(\"#count >0.2 :\",count/beam_width)        \n",
    "        \n",
    "# need data_in, data_out, Enc_in Dec_out\n",
    "def for_humain_eval(i,n,pt,char2tone_dict,indices,om,forced_tone=True,forced_word=False,beam_width=1):\n",
    "    encoder = torch.load(\"./models/II_enc_epoch_\"+pt+\".pt\",map_location='cpu')\n",
    "    attn_decoder = torch.load(\"./models/II_dec_epoch_\"+pt+\".pt\",map_location='cpu')\n",
    "    encoder.eval()\n",
    "    attn_decoder.eval()\n",
    "    print(\"encoder.hidden_size:\",encoder.hidden_size)\n",
    "    print(\"attn_decoder.hidden_size:\",attn_decoder.hidden_size)\n",
    "\n",
    "    f = open(\"./random_\"+str(i)+\"_\"+pt+\"_\"+str(forced_tone)+\"_\"+str(forced_word)+\"_\"+str(beam_width)+\".txt\",\"w\")\n",
    "    f_oomm = open(\"./random_oomm_\"+str(i)+\"_\"+pt+\"_\"+str(forced_tone)+\"_\"+str(forced_word)+\"_\"+str(beam_width)+\".txt\",\"w\")\n",
    "    f_a = open(\"./random_notes_\"+str(i)+\"_\"+pt+\"_\"+str(forced_tone)+\"_\"+str(forced_word)+\"_\"+str(beam_width)+\".txt\",\"w\")\n",
    "    \n",
    "    \n",
    "    f_a.write(\"### oo => 0, mm => 1 ###\")\n",
    "    f_a.write(\"\\n\")\n",
    "    \n",
    "    for idx in range(n):\n",
    "        \n",
    "        f_a.write(\"-Question-\"+str(idx+1)+\"\\n\")\n",
    "        f_a.write(\"Indices:\"+str(indices[idx])+\"\\n\")\n",
    "        f_a.write(\"Truth:\"+str(om[idx])+\"\\n\")\n",
    "        f_a.write(\"\\n\")\n",
    "    \n",
    "        # generate model answer\n",
    "        i = indices[idx]\n",
    "        pair = torch.Tensor(Enc_in[i]).to(torch.int64).unsqueeze(0)\n",
    "        d_out = torch.Tensor(Dec_in[i]).to(torch.int64).unsqueeze(0)\n",
    "        output_words, _ = evaluate(char2tone_dict,encoder, attn_decoder, pair,forced_tone,forced_word,beam_width)\n",
    "        \n",
    "        # without oomm \n",
    "        f.write(\"-Question-\"+str(idx+1)+\"\\n\")\n",
    "        f.write(\"Indices:\"+str(i)+\"\\n\")\n",
    "        f.write(data_in[i]+\" ii\\n\")\n",
    "        if(om[idx] == 0):\n",
    "            f.write(data_out[i]+\"\\n\")\n",
    "            f.write(output_words[0]+\"\\n\")\n",
    "        else:\n",
    "            f.write(output_words[0]+\"\\n\")\n",
    "            f.write(data_out[i]+\"\\n\")\n",
    "        f.write('\\n')\n",
    "        \n",
    "        # with oomm    \n",
    "        f_oomm.write(\"-Question-\"+str(idx+1)+\"\\n\")\n",
    "        f_oomm.write(\"Indices:\"+str(i)+\"\\n\")\n",
    "        f_oomm.write(data_in[i]+\" ii\\n\")   \n",
    "        if(om[idx] == 0): \n",
    "            f_oomm.write(data_out[i]+\" oo\\n\")\n",
    "            f_oomm.write(output_words[0]+\" mm\\n\")\n",
    "        else:\n",
    "            f_oomm.write(output_words[0]+\" mm\\n\")\n",
    "            f_oomm.write(data_out[i]+\" oo\\n\")            \n",
    "        f_oomm.write('\\n')\n",
    "        \n",
    "    f_a.close\n",
    "    f.close\n",
    "    f_oomm.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Evaluation\n",
    "Prepare couplets to be evaluated latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_path = \"./test_in.txt\"\n",
    "out_path = \"./test_out.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load couplets data and create necessary tools for the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### data preparing ### \n",
      "Batch size is:  1\n",
      "Use all couplets to train:  True\n",
      "### data loading ### \n",
      "min line length is  1\n",
      "max line length is  32\n",
      "Loaded input couplets length is  3931\n",
      "Loaded output couplets length is  3931\n",
      "Vocab length is  9131\n",
      "Number of couplet batches : 3931\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "data_in,data_out,vocab,max_length = prepare_data(in_path,out_path,with_all_couplets=True)\n",
    "char2int_dict, int2char_dict, char2tone_dict, int2tone_dict = create_dicts(data_in,data_out,vocab,max_length)\n",
    "tone_onlyP_2, tone_onlyZ_2 = create_tone_mask(vocab, char2tone_dict)\n",
    "Enc_in,Dec_in,Dec_out = tokenization(data_in,data_out,char2int_dict,max_length)\n",
    "batch_in_out_pairs_eval = prepare_batches(Enc_in,Dec_out,Dec_in,batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Encoder and Decoder from `models` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = torch.load(\"./models/II_enc_epoch_203.pt\",map_location='cpu')\n",
    "attn_decoder = torch.load(\"./models/II_dec_epoch_203.pt\",map_location='cpu')\n",
    "encoder.eval()\n",
    "attn_decoder.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute bleu score for models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## FIN ##\n",
      "BLEU : 0.13963461478725833\n",
      "#count >0.2 : 1178.0\n"
     ]
    }
   ],
   "source": [
    "# 203\n",
    "compute_bleu(char2tone_dict,encoder, attn_decoder,forced_tone=True,forced_word=True,beam_width=1,n=len(batch_in_out_pairs_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## FIN ##\n",
      "BLEU : 0.1380228152769069\n",
      "#count >0.2 : 1151.3333333333333\n"
     ]
    }
   ],
   "source": [
    "# 203\n",
    "compute_bleu(char2tone_dict,encoder, attn_decoder,forced_tone=True,forced_word=True,beam_width=3,n=len(batch_in_out_pairs_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## FIN ##\n",
      "BLEU : 0.13732333521721898\n",
      "#count >0.2 : 1146.6\n"
     ]
    }
   ],
   "source": [
    "# 203\n",
    "compute_bleu(char2tone_dict,encoder, attn_decoder,forced_tone=True,forced_word=True,beam_width=5,n=len(batch_in_out_pairs_eval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate randomly 10 couplets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: \n",
      "湖山一览，无限情怀；不由我胸中纳秋水落霞，欲与高天争旷阔\n",
      "model answer: \n",
      "岁月千年，有心意趣皆共赏梅台雅兴，且同雅，待朋朋友谊，且\n",
      "desired answer: \n",
      "楼阁重新，倍增光彩；更待谁笔底掀长江后浪，敢同圣序比风流\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xzou/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:367: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluateRandomly(char2tone_dict,encoder, attn_decoder,forced_tone=True,forced_word=True,beam_width=1,n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
